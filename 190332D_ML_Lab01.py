# -*- coding: utf-8 -*-
"""190332D_ML_Lab01.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jDCCZ4pDId2P_qEn4ndlacHBfqUOT17X

# Loading Data
"""

# Mounting Google Drive to access access
from google.colab import drive
drive.mount('/content/drive')

# Loading data
import pandas as pd
train = pd.read_csv("/content/drive/My Drive/Semester7/ML/Lab01/train.csv")
valid = pd.read_csv("/content/drive/My Drive/Semester7/ML/Lab01/valid.csv")
test = pd.read_csv("/content/drive/My Drive/Semester7/ML/Lab01/test.csv")

train.head()

"""# Writing to CSV"""

x_test = test.iloc[:, : -4]
x_test.head()

# Function to write to CSV
def create_csv(x, y_1, y_2, label_name):
    output_filename = f"/content/drive/My Drive/Semester7/ML/Lab01/190332D_{label_name}.csv"
    combined_data = pd.DataFrame()

    combined_data["Predicted labels before feature engineering"] = y_1
    combined_data["Predicted labels after feature engineering"] = y_2
    combined_data["No of new features"] = [len(x.columns)] * len(combined_data)

    i = 0;
    while i < len(x.columns):
        column_name = x.columns[i]
        combined_data[f"new_feature_{i+1}"] = x[column_name]
        i += 1

    while i < 256:
        combined_data[f"new_feature_{i+1}"] = [None] * len(combined_data)
        i += 1

    combined_data.to_csv(output_filename, index=False)

"""# Handling Missing Values"""

# Check for missing values in each label
labels = ["label_1", "label_2", "label_3", "label_4"]
train[labels].isnull().sum()

"""The `label_2` contains 480 missing values. Therefore, fill these with the mean of the column."""

from sklearn.impute import SimpleImputer

imputer = SimpleImputer(strategy='mean')
columns = ["label_2"]
imputer.fit(train[columns])

# Insert the missing values with the mean with the nearest integer
train[columns] = imputer.transform(train[columns]).round().astype("int")

train.head()

# Recheck for missing values in each label to confirm whether there are no missing values left
labels = ["label_1", "label_2", "label_3", "label_4"]
train[labels].isnull().sum()

"""Do the same procedure for validation dataset."""

valid[labels].isnull().sum()

imputer = SimpleImputer(strategy='mean')
columns = ["label_2"]
imputer.fit(valid[columns])

# Insert the missing values with the mean with the nearest integer
valid[columns] = imputer.transform(valid[columns]).round().astype("int")

valid.head()

valid[labels].isnull().sum()

"""# Modelling `label_1`: Speaker ID

Speaker ID can be considered as a categorical variable.
"""

# Split X and Y
x = train.iloc[:, : -4]
y = train["label_1"]
x_valid = valid.iloc[:, : -4]
y_valid = valid["label_1"]

x.head()

y.head()

"""## Create a KNN Model"""

from sklearn.neighbors import KNeighborsClassifier
from math import sqrt
from sklearn.metrics import accuracy_score

# K value for KNN model is chosen as the 4th root of the dataset size
k = int(sqrt(sqrt(len(x))))
print(k)

# Create the KNN nodel
knn_label_1 = KNeighborsClassifier(n_neighbors = k)
knn_label_1.fit(x, y)

y_pred_label_1 = knn_label_1.predict(x_valid)
accuracy_label_1 = accuracy_score(y_valid, y_pred_label_1)
print(accuracy_label_1)

"""## Check for Columns with Low Variance"""

from sklearn.feature_selection import VarianceThreshold

var_thres = VarianceThreshold(threshold = 0.3)
var_thres.fit(x)

constant_columns = [column for column in x.columns
                    if column not in x.columns[var_thres.get_support()]]

print(len(constant_columns))

"""There are no columns with low variance to be ignored.

## Check for Correlation between features
"""

correlated_features = set()

correlation_matrix = x.corr()
for i in range(len(x.columns)):
    for j in range(i):
        if abs(correlation_matrix.iloc[i, j]) > 0.7:
            colname = correlation_matrix.columns[i]
            correlated_features.add(colname)

len(set(correlated_features))

"""There are no tightly correlated features to be merged.

## Check for Mutual Information
"""

from sklearn.feature_selection import mutual_info_classif, SelectKBest

# Calculate Mutual Information
mi = pd.Series(mutual_info_classif(x, y))
mi.sort_values(ascending = False)

mi.sort_values(ascending=False).plot.bar(figsize=(32, 12))

"""Since there are features with higher impact as well as comparatively lesser impact on y value, using feature selection with best features. Taking the count of mutual information value higher than 0.2 by observing the graph.

## Feature Selection
"""

no_of_selected_features = mi[mi > 0.2].count()
print (f"Selected {no_of_selected_features} features out of {len(mi)} features.")

"""There are only 72 features with mutual information value higher than 0.2 for `label_1`. Ignore the rest meantime."""

# Take a copy of x
x_label_1 = x.copy(deep = True)
x_valid_label_1 = x_valid.copy(deep = True)

# Select only the features with maximum mutual information value
selected_cols_label_1 = SelectKBest(mutual_info_classif, k = no_of_selected_features)
selected_cols_label_1.fit(x_label_1, y)
selected_col_names_label_1 = x_label_1.columns[selected_cols_label_1.get_support()]
print(selected_col_names_label_1)

x_label_1 = pd.DataFrame(x_label_1, columns = selected_col_names_label_1)
x_valid_label_1 = pd.DataFrame(x_valid_label_1, columns = selected_col_names_label_1)

"""## Create the KNN Model again"""

knn_label_1_selected = KNeighborsClassifier(n_neighbors = k)
knn_label_1_selected.fit(x_label_1, y)

y_pred_label_1_selected = knn_label_1_selected.predict(x_valid_label_1)
accuracy_label_1_selected = accuracy_score(y_valid, y_pred_label_1_selected)
print(f"Accuracy of the new model is {accuracy_label_1_selected} and the accuracy of the original model was {accuracy_label_1}.")

"""## Predicting and writing to CSV"""

x_test_label_1 = x_test.copy(deep = True)
x_test_label_1 = pd.DataFrame(x_test_label_1, columns = selected_col_names_label_1)

y_test_pred_label_1 = knn_label_1.predict(x_test)
y_test_pred_label_1_selected = knn_label_1_selected.predict(x_test_label_1)
create_csv(x_test_label_1, y_test_pred_label_1, y_test_pred_label_1_selected, "label_1")

"""# Modelling `label_2`: Speaker Age

Speaker age can be considered as a numerical variable and hence is a regression problem.

## Create the KNN Model
"""

from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import mean_squared_error

y = train["label_2"]
y_valid = valid["label_2"]

# Create the KNN nodel
knn_label_2 = KNeighborsRegressor(n_neighbors = k)
knn_label_2.fit(x, y)

y_pred_label_2 = knn_label_2.predict(x_valid)
mse_label_2 = mean_squared_error(y_valid, y_pred_label_2)
print(mse_label_2)

"""## Check for Mutual Information"""

from sklearn.feature_selection import mutual_info_regression, SelectPercentile

mi = pd.Series(mutual_info_regression(x, y))
mi.sort_values(ascending=False)

mi.sort_values(ascending=False).plot.bar(figsize=(32, 12))

"""The distribution is similar to `label_1`. Taking the mutual information values higher than 0.06.

## Feature Selection
"""

no_of_selected_features = mi[mi > 0.06].count()
print (f"Selected {no_of_selected_features} features out of {len(mi)} features.")

"""There are only 87 features with the mutual information value greater than 0.06. Ignore the rest."""

# Take a copy of x
x_label_2 = x.copy(deep = True)
x_valid_label_2 = x_valid.copy(deep = True)

# Select only the features with maximum mutual information value
selected_cols_label_2 = SelectKBest(mutual_info_classif, k = no_of_selected_features)
selected_cols_label_2.fit(x_label_2, y)
selected_col_names_label_2 = x_label_2.columns[selected_cols_label_2.get_support()]
print(f"Selected {len(selected_col_names_label_2)} features out of {len(mi)} features.")
print(selected_col_names_label_2)

x_label_2 = pd.DataFrame(x_label_2, columns = selected_col_names_label_2)
x_valid_label_2 = pd.DataFrame(x_valid_label_2, columns = selected_col_names_label_2)

"""## Create the KNN Model again"""

# Create the KNN nodel
knn_label_2_selected = KNeighborsRegressor(n_neighbors = k)
knn_label_2_selected.fit(x_label_2, y)

y_pred = knn_label_2_selected.predict(x_valid_label_2)
mse_label_2_selected = mean_squared_error(y_valid, y_pred)
print(f"Mean squared error of the new model is {mse_label_2_selected} and the MSE of the original model was {mse_label_2}.")

"""## Predicting and writing to CSV"""

x_test_label_2 = x_test.copy(deep = True)
x_test_label_2 = pd.DataFrame(x_test_label_2, columns = selected_col_names_label_2)

y_test_pred_label_2 = knn_label_2.predict(x_test)
y_test_pred_label_2_selected = knn_label_2_selected.predict(x_test_label_2)
create_csv(x_test_label_2, y_test_pred_label_2, y_test_pred_label_2_selected, "label_2")

"""# Modelling `label_3`: Speaker Gender

Speaker gender is a categorical variable.

## Create the Random Forest Model
"""

y = train["label_3"]
y_valid = valid["label_3"]

from sklearn.ensemble import RandomForestClassifier

# Create the Random Forest model
rf_label_3 = RandomForestClassifier(n_estimators=100, random_state=42)
rf_label_3.fit(x, y)

y_pred_label_3 = rf_label_3.predict(x_valid)
accuracy_label_3 = accuracy_score(y_valid, y_pred_label_3)
print(accuracy_label_3)

"""## Check for Mutual Information"""

mi = pd.Series(mutual_info_classif(x, y))
mi.sort_values(ascending = False)

mi.sort_values(ascending=False).plot.bar(figsize=(32, 12))

"""There are a smaller number of features that have a high impact on y value. Using features with mutual information value greater than 0.05 observing the graph.

## Feature Selection
"""

no_of_selected_features = mi[mi > 0.05].count()
print (f"Selected {no_of_selected_features} features out of {len(mi)} features.")

"""There are only 37 features with mutual information value higher than 0.05 for `label_3`. Ignore the rest meantime."""

x_label_3 = x.copy(deep = True)
x_valid_label_3 = x_valid.copy(deep = True)

selected_cols_label_3 = SelectKBest(mutual_info_classif, k = no_of_selected_features)
selected_cols_label_3.fit(x_label_3, y)
selected_col_names_label_3 = x_label_3.columns[selected_cols_label_3.get_support()]
print(selected_col_names_label_3)

x_label_3 = pd.DataFrame(x_label_3, columns = selected_col_names_label_3)
x_valid_label_3 = pd.DataFrame(x_valid_label_3, columns = selected_col_names_label_3)

"""## Create the RF Model again"""

rf_label_3_selected = RandomForestClassifier(n_estimators = 100, random_state = 42)
rf_label_3_selected.fit(x_label_3, y)

y_pred = rf_label_3_selected.predict(x_valid_label_3)
accuracy_label_3_selected = accuracy_score(y_valid, y_pred)
print(f"Accuracy of the new model is {accuracy_label_3_selected} and the accuracy of the original model was {accuracy_label_3}.")

"""## Create XGBoost Model

Try the XGBoost model for comparing.
"""

import xgboost as xgb

# Create the XGBoost model
xgb_label_3 = xgb.XGBClassifier(n_estimators = 50, random_state = 42)
xgb_label_3.fit(x, y)

y_pred = xgb_label_3.predict(x_valid)
accuracy_label_3 = accuracy_score(y_valid, y_pred)
print(accuracy_label_3)

# Create the XGBoost model for selected features
xgb_label_3_selected = xgb.XGBClassifier(n_estimators = 50, random_state = 478)
xgb_label_3_selected.fit(x_label_3, y)

y_pred = xgb_label_3_selected.predict(x_valid_label_3)
accuracy_label_3_selected = accuracy_score(y_valid, y_pred)
print(f"Accuracy of the new model is {accuracy_label_3_selected} and the accuracy of the original model was {accuracy_label_3}.")

"""The XGBoost model resulted in slightly higher accuracy for the validation dataset than the Random Forest model.

## Predict and write test to CSV
"""

x_test_label_3 = x_test.copy(deep = True)
x_test_label_3 = pd.DataFrame(x_test_label_3, columns = selected_col_names_label_3)

y_test_pred_label_3 = xgb_label_3.predict(x_test)
y_test_pred_label_3_selected = xgb_label_3_selected.predict(x_test_label_3)
create_csv(x_test_label_3, y_test_pred_label_3, y_test_pred_label_3_selected, "label_3")

"""# Modelling `label_4`: Speaker Accent

Speaker accent is a categorical variable.
"""

y = train["label_4"]
y_valid = valid["label_4"]

from collections import Counter

Counter(y)

import matplotlib.pyplot as plt

label_counts = y.value_counts()

plt.figure(figsize=(12, 6))
label_counts.plot(kind='bar')
plt.xlabel('Value')
plt.ylabel('Count')
plt.title('Label Distribution')
plt.xticks(rotation=45)
plt.show()

"""The value 6 has comparatively a vast number of occurences than any other value. Since the `label_4` is not evenly distributed, we have to use sampling.

## Undersampling

Firstly use undersampling and see the accuracy of the model.
"""

from imblearn.under_sampling import RandomUnderSampler

undersampler = RandomUnderSampler(sampling_strategy='all', random_state=42)
x_undersampled, y_undersampled = undersampler.fit_resample(x, y)
Counter(y_undersampled)

"""### Create the KNN Model"""

knn_label_4_undersampled = KNeighborsClassifier(n_neighbors = k)
knn_label_4_undersampled.fit(x_undersampled, y_undersampled)

y_pred = knn_label_4_undersampled.predict(x_valid)
accuracy_label_4_undersampled = accuracy_score(y_valid, y_pred)
print(accuracy_label_4_undersampled)

"""The accuracy of the model is not enough.

### Check for Mutual Information
"""

mi = pd.Series(mutual_info_classif(x_undersampled, y_undersampled))
mi.sort_values(ascending = False)

mi.sort_values(ascending=False).plot.bar(figsize=(32, 12))

"""Observing the graph, take the features with mutual information value higher than 0.1."""

no_of_selected_features = mi[mi > 0.1].count()
print (f"Selected {no_of_selected_features} features out of {len(mi)} features.")

x_label_4_undersampled = x_undersampled.copy(deep = True)
x_valid_label_4_undersampled = x_valid.copy(deep = True)

selected_cols = SelectKBest(mutual_info_classif, k = no_of_selected_features)
selected_cols.fit(x_label_4_undersampled, y_undersampled)
selected_col_names = x_label_4_undersampled.columns[selected_cols.get_support()]
print(selected_col_names)

"""### Create the KNN Model again"""

x_label_4_undersampled = pd.DataFrame(x_label_4_undersampled, columns = selected_col_names)
x_valid_label_4_undersampled = pd.DataFrame(x_valid_label_4_undersampled, columns = selected_col_names)

knn_label_4_undersampled_selected = KNeighborsClassifier(n_neighbors = k)
knn_label_4_undersampled_selected.fit(x_label_4_undersampled, y_undersampled)

y_pred = knn_label_4_undersampled_selected.predict(x_valid_label_4_undersampled)
accuracy_label_4_undersampled_selected = accuracy_score(y_valid, y_pred)
print(f"Accuracy of the new model is {accuracy_label_4_undersampled_selected} and the accuracy of the original model was {accuracy_label_4_undersampled}.")

"""The accuracy further decreased with feature selection. The model is not suitable.

*Edit: With the new run, the accuracy has slightly increased after feature selection.*

## Oversampling
"""

from imblearn.over_sampling import RandomOverSampler

oversampler = RandomOverSampler(sampling_strategy='all', random_state=42)
x_oversampled, y_oversampled = oversampler.fit_resample(x, y)
Counter(y_oversampled)

"""### Create the KNN Model"""

knn_label_4_oversampled = KNeighborsClassifier(n_neighbors = k)
knn_label_4_oversampled.fit(x_oversampled, y_oversampled)

y_pred_label_4 = knn_label_4_oversampled.predict(x_valid)
accuracy_label_4_oversampled = accuracy_score(y_valid, y_pred_label_4)
print(accuracy_label_4_oversampled)

"""The accuracy of the oversampling is acceptable.

Check for Mutual Information
"""

mi = pd.Series(mutual_info_classif(x_oversampled, y_oversampled))
mi.sort_values(ascending = False)

mi.min()

mi.sort_values(ascending=False).plot.bar(figsize=(32, 12))

"""The mutual information graph has an almost uniform distribution. Therefore, it is not possible to select best features. Using PCA to reduce the dimension.

### Use Principal Component Analysis
"""

x_label_4_oversampled = x_oversampled.copy(deep = True)
x_valid_label_4_oversampled = x_valid.copy(deep = True)

from sklearn.decomposition import PCA

pca = PCA(n_components = 0.95, svd_solver = 'full')
pca.fit(x_label_4_oversampled)
x_label_4_trf = pd.DataFrame(pca.transform(x_label_4_oversampled))
x_valid_label_4_trf = pd.DataFrame(pca.transform(x_valid_label_4_oversampled))
x_label_4_trf.shape

"""Now there are only 64 features."""

knn_label_4_oversampled_selected = KNeighborsClassifier(n_neighbors = k)
knn_label_4_oversampled_selected.fit(x_label_4_trf, y_oversampled)

y_pred = knn_label_4_oversampled_selected.predict(x_valid_label_4_trf)
accuracy_label_4_oversampled_selected = accuracy_score(y_valid, y_pred)
print(f"Accuracy of the new model is {accuracy_label_4_oversampled_selected} and the accuracy of the original model was {accuracy_label_4_oversampled}.")

"""### Predicting and writing test data to CSV"""

x_test_label_4 = x_test.copy(deep = True)
x_test_label_4 = pd.DataFrame(pca.transform(x_test_label_4))

y_test_pred_label_4 = knn_label_4_oversampled.predict(x_test)
y_test_pred_label_4_selected = knn_label_4_oversampled_selected.predict(x_test_label_4)
create_csv(x_test_label_4, y_test_pred_label_4, y_test_pred_label_4_selected, "label_4")